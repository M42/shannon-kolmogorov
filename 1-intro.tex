% Author: Nicolò Cesa-Bianchi (2016)
% This document is licensed under Creative Commons BY-NC-SA.
\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage[spanish]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}

\newtheorem{theorem}{Teorema}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{defi}[theorem]{Definizione}
\newtheorem{cor}[theorem]{Corollario}
\newtheorem{example}[theorem]{Esempio}
\newtheorem{fact}[theorem]{Fatto}

\newcommand{\figscale}[2]{\includegraphics[scale=#2,clip=false]{#1}}

\newlength{\parunit}
\setlength{\parunit}{3mm}

\setlength{\parindent}{0pt}
\setlength{\parskip}{\parunit}
\setlength{\belowdisplayskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}

\newcommand{\ssep}{\\[1.5\parunit]}
\newcommand{\msep}{\\[2.0\parunit]}

\newcommand{\scO}{\mathcal{O}}
\newcommand{\bg}{ \boldsymbol{g} }
\newcommand{\bu}{ \boldsymbol{u} }
\newcommand{\bv}{ \boldsymbol{v} }
\newcommand{\bw}{ \boldsymbol{w} }
\newcommand{\bx}{ \boldsymbol{x} }
\newcommand{\by}{ \boldsymbol{y} }
\newcommand{\bX}{ \boldsymbol{X} }
\newcommand{\bY}{ \boldsymbol{Y} }
\newcommand{\bz}{ \boldsymbol{z} }
\newcommand{\be}{ \boldsymbol{e} }
\newcommand{\bone}{ \boldsymbol{1} }
\newcommand{\bzero}{ \boldsymbol{0} }
\newcommand{\bphi}{ \boldsymbol{\phi} }
\newcommand{\bxi}{ \boldsymbol{\xi} }
\newcommand{\bsigma}{ \boldsymbol{\sigma} }
\newcommand{\bbw}{\overline{\bw}}
\newcommand{\scD}{\mathcal{D}}
\newcommand{\scH}{\mathcal{H}}
\newcommand{\scF}{\mathcal{F}}
\newcommand{\scM}{\mathcal{M}}
\newcommand{\scP}{\mathcal{P}}
\newcommand{\scR}{\mathcal{R}}
\newcommand{\scX}{\mathcal{X}}
\newcommand{\scY}{\mathcal{Y}}
\newcommand{\yhat}{\widehat{y}}
\newcommand{\Yhat}{\widehat{Y}}
\newcommand{\hhat}{\widehat{h}}
\newcommand{\fhat}{\widehat{f}}
\newcommand{\defeq}{\stackrel{\rm def}{=}}
\newcommand{\ol}{\overline}
\newcommand{\loss}{\ell}
\newcommand{\er}{\mathrm{er}}
\newcommand{\sgn}{\mathrm{sgn}}
\renewcommand{\ss}{\subseteq}
\newcommand{\bool}{\{0,1\}}
\newcommand{\spin}{\{-1,+1\}}
\newcommand{\norm}[1]{ \left\|{#1}\right\| }
\newcommand{\theset}[2]{ \left\{ {#1} \,:\, {#2} \right\} }
\newcommand{\field}[1]{\mathbb{#1}}
\renewcommand{\Pr}{\field{P}}
\newcommand{\R}{\field{R}}
\newcommand{\E}{\field{E}}
\newcommand{\Ind}[1]{\field{I}{\{{#1}\}}}
\newcommand{\ve}{\varepsilon}
\newcommand{\dt}{\displaystyle}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}


\begin{document}

\begin{center}
\fbox{
\begin{minipage}{0.9\textwidth}
\textbf{Teoría de la Información y de la Transmisión}
\begin{center}
\textbf{\Large Introducción}
\end{center}
Docente: \textit{Nicolò Cesa-Bianchi} \\ 
Traducción: \textit{Mario Román} \\
Licencia: Creative Commons BY-SA-NC \hfill versión \today 
\end{minipage}
}
\end{center}

\bigskip

La teoría de la información está en la base de importantes sectores de la informática y las telecomunicaciones como la compresión y la transmisión de datos, el aprendizaje automático y la criptografía. Las principales teorías formales del concepto de información son dos:
\begin{enumerate}
\item La de Claude Shannon (USA, 1916--2001) formulada en el 1948 y basada en la teoría de la probabilidad.
\item La de Andrey Kolmogorov (URSS, 1903--1987) formulada en el 1965 y basada en la teoría de la computación. La misma teoría fue propuesta independientemente también por Gregory Chaitin (USA, 1947--\ ) y Ray Solomonoff (USA, 1926--2009), 
\end{enumerate}
En cierto sentido, la teoría de Kolmogorov es más astracta que la de Shannon, que ---por el contrario--- tiene una mayor relevancia aplicativa.


La teoría de la información de Shannon fue formulada en su artículo ``A Mathematical Theory of Communication'', publicado por Bell Labs en el 1948. El problema que la teoría de Shannon afronta es el de transmitir información a través de un canal afectado por el ruido.

\begin{figure}[h]
\begin{center}
\figscale{Images/canale}{0.5}
\end{center}
\caption{
\label{fig:canale}
Modelo de un canal con ruido.
}
\end{figure}

De manera más precisa, se busca codificar los mensajes de forma que se maximice la información transmitida en cada uso del canal, minimizando simultáneamente los errores de transmisión debidos a la presencia de ruido. El problema es que la información no está distribuida de manera uniforme en el mensaje. En consecuencia, es difícil comprobar los errores de transmisión. La metodología ideada por Shannon consiste en separar el problema en dos subproblemas que pueden afrontarse separadamente: primero codifico de manera que concentre la información eliminando la redundancia del mensaje, después, recodifico con el fin de maximizar los errores de transmisión.

En la práctica, nos hace falta entonces desarrollar dos codificaciones separadas:
\begin{enumerate}
\item \textbf{Source Coding} (I teorema de Shannon). Código para representar el mensaje de forma que maximice la compresión (por ejemplo, \texttt{ZIP}).
\item \textbf{Channel Coding} (II teorema de Shannon). Código para representar el mensaje de forma que minimice los errores de trasmisión.
\end{enumerate}
La codificación fuente (source coding) elimina la mayor parte de la redundancia del mensaje, concentrando la información. La codificación del canal (channel coding) reintroduce redundancia de manera controlada para contrarrestar la distorsión producida por el ruido.

El primer y el segundo teorema de Shannon vienen entonces combinados para demostrar el teorema de codificación conjunta fuente-canal. Este teorema demuestra que la codificación en dos pasos, donde primero comprimimos la fuente y después creamos un código canal para transmitir los símbolos comprimidos, es óptima. Esto no es trivial, de hecho, el código fuente ignora al canal, mientras el código canal ignora la fuente. El teorema, sin embargo, demuestra que no existe forma de tener una codificación más eficiente incluso considerando simultáneamente fuente y canal.

\end{document}
