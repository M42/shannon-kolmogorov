% Author: Nicolò Cesa-Bianchi (2016)
% This document is licensed under Creative Commons BY-NC-SA.
\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage[spanish]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}

\newtheorem{theorem}{Teorema}
\newtheorem{lemma}[theorem]{Lema}
\newtheorem{defi}[theorem]{Definición}
\newtheorem{cor}[theorem]{Corolario}
\newtheorem{example}[theorem]{Ejemplo}
\newtheorem{fact}[theorem]{Proposición}

\newcommand{\figscale}[2]{\includegraphics[scale=#2,clip=false]{#1}}

\newlength{\parunit}
\setlength{\parunit}{3mm}

\setlength{\parindent}{0pt}
\setlength{\parskip}{\parunit}
\setlength{\belowdisplayskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}

\newcommand{\ssep}{\\[1.5\parunit]}
\newcommand{\msep}{\\[2.0\parunit]}

\newcommand{\scO}{\mathcal{O}}
\newcommand{\bg}{ \boldsymbol{g} }
\newcommand{\bu}{ \boldsymbol{u} }
\newcommand{\bv}{ \boldsymbol{v} }
\newcommand{\bw}{ \boldsymbol{w} }
\newcommand{\bx}{ \boldsymbol{x} }
\newcommand{\by}{ \boldsymbol{y} }
\newcommand{\bX}{ \boldsymbol{X} }
\newcommand{\bY}{ \boldsymbol{Y} }
\newcommand{\bz}{ \boldsymbol{z} }
\newcommand{\be}{ \boldsymbol{e} }
\newcommand{\bone}{ \boldsymbol{1} }
\newcommand{\bzero}{ \boldsymbol{0} }
\newcommand{\bphi}{ \boldsymbol{\phi} }
\newcommand{\bxi}{ \boldsymbol{\xi} }
\newcommand{\bsigma}{ \boldsymbol{\sigma} }
\newcommand{\bbw}{\overline{\bw}}
\newcommand{\scD}{\mathcal{D}}
\newcommand{\scH}{\mathcal{H}}
\newcommand{\scF}{\mathcal{F}}
\newcommand{\scM}{\mathcal{M}}
\newcommand{\scP}{\mathcal{P}}
\newcommand{\scR}{\mathcal{R}}
\newcommand{\scX}{\mathcal{X}}
\newcommand{\scY}{\mathcal{Y}}
\newcommand{\yhat}{\widehat{y}}
\newcommand{\Yhat}{\widehat{Y}}
\newcommand{\hhat}{\widehat{h}}
\newcommand{\fhat}{\widehat{f}}
\newcommand{\defeq}{\stackrel{\rm def}{=}}
\newcommand{\ol}{\overline}
\newcommand{\loss}{\ell}
\newcommand{\er}{\mathrm{er}}
\newcommand{\sgn}{\mathrm{sgn}}
\renewcommand{\ss}{\subseteq}
\newcommand{\bool}{\{0,1\}}
\newcommand{\spin}{\{-1,+1\}}
\newcommand{\norm}[1]{ \left\|{#1}\right\| }
\newcommand{\theset}[2]{ \left\{ {#1} \,:\, {#2} \right\} }
\newcommand{\field}[1]{\mathbb{#1}}
\renewcommand{\Pr}{\field{P}}
\newcommand{\R}{\field{R}}
\newcommand{\N}{\field{N}}
\newcommand{\E}{\field{E}}
\newcommand{\Ind}[1]{\field{I}{\{{#1}\}}}
\newcommand{\ve}{\varepsilon}
\newcommand{\dt}{\displaystyle}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}


\begin{document}

\begin{center}
\fbox{
\begin{minipage}{0.9\textwidth}
\textbf{Teoría de la Información y de la Transmisión}
\begin{center}
\textbf{\Large Codificación de la fuente: definiciones}
\end{center}
Docente: \textit{Nicolò Cesa-Bianchi} \\ 
Traducción: \textit{Mario Román} \\
Licencia: Creative Commons BY-SA-NC \hfill versión \today 
\end{minipage}
}
\end{center}

\bigskip

Los mensajes a transmitir son generados desde una entidad abstracta llamada fuente. Sea $\scX$ el conjunto finito de símbolos que componen los mensajes generados por la fuente. Un mensaje $\bx = (x_1,\dots,x_n) \in \scX^n$ de longitud $n$ es una secuencia de $n$ símbolos fuente. Una función de codificación leva símbolos fuente a palabras de código. Una palabra de código es una secuencia de números del conjunto $\{0,\dots,D-1\}$ de los símbolos de código, donde $D > 1$ es la base del código. Por ejemplo, con $D=2$ obtenemos los códigos binarios. En este sentido, podemos representar una función de codificación por un código fuente con

\[
    c : \scX \to \{0,\dots,D-1\}^+
\]
donde $\{0,\dots,D-1\}^+$ representa el conjunto de las secuencias sobre $\{0,\dots,D-1\}$ de longitud mayor o igual a uno. Formalmente,
\[
    \{0,\dots,D-1\}^+ = \bigcup_{n=1}^{\infty} \{0,\dots,D-1\}^n~.
\]
Por ejemplo, $(2,1)$ y $(4,7,3)$ pertenecen ambas a $\{0,\dots,D-1\}^+$ con $D=8$.
%
\begin{example}
\label{ex:1}
Dado $\scX = \{\heartsuit,\diamondsuit,\clubsuit,\spadesuit\}$, un ejemplo de código binario $c : \scX\to\{0,1\}^+$ es el siguiente:
\[
    c(\heartsuit) = 0 \quad c(\diamondsuit) = 010 \quad c(\clubsuit) = 01 \quad c(\spadesuit) = 10~.
\]
\end{example}
%
Dado que el objetivo de un código fuente es maximizar la compresión, estamos interesados en medir la cantidad $\ell_c(x)$ definida como la longitud de la palabra de código $c(x)$ para el símbolo $x\in\scX$. En el ejemplo precedente, $\ell_c(\diamondsuit) = 3$.
%Definiamo anche la lunghezza totale della codifica di $\bx = (x_1,\dots,x_n) \in \scX^n$ come
%\[
%    L_c(\bx) = \sum_{i=1}^n \ell_c(x_i)~.
%\]
El objetivo de un código fuente es el de minimizar la longitud media de la palabra de código utilizada para codificar los símbolos fuente.

La intución en la base de la construcción de códigos fuente es la misma del alfabeto Morse: utilizar palabras de código cortas para los símbolos que son generados frecuentemente por la fuente. Para poder analizar de modo riguroso debemos crear un modelo formal de la fuente. La propuesta de Shannon es la de definir una distribución de probabilidad $p$ fijada sobre símbolos fuente y entonces asumir que $p(x)$ represente la probabilidad de que la fuente genere el símbolo $x\in\scX$. Un \textbf{modelo de fuente} está entonces definido por la pareja $\langle\scX,p\rangle$.

Nótese que, en realidad, la cantidad interesante aquí es la distribución de probabilidad sobre los mensajes $\bx$ más que sobre los símbolos $x$, dado que los mensajes son los objetos que queremos transmitir. Dada una distribución $p$ sobre símbolos $\scX$ definimos entonces una distribución $P_n$ sobre los mensajes $\scX^n$ de longitud $n$ como
\[
    P_n(x_1,\dots,x_n) = p(x_1) \times\cdots\times p(x_n)~.
\]
Esta definición de $P_n$ corresponde a asumir que la fuente genera un mensaje a través de extracciones indipendendientes de símbolos. En generale, sin embargo, esta asunción no es muy plausibile. Por ejemplo, pensemos en un mensaje de texto en castellano donde los símbolos fuente son las letras del alfabeto incluyendo espacios y punto. Claramente, hay fuertes dependencias entre una letra del mensaje y las letras que están alrededor y tales dependencias no son capturadas por la $P_n$ definida como antes. Por otro lado, el análisis matematico se ve muy facilitado por la asunción de independencia. En lo que sigue, asumiremos entonces la independencia de los símbolos fuente, teniendo sin embargo en mente que códigos fuente más realistas y sofisticados pueden obtenerse sin esta asunción.

De ahora en adelante identificamos un símbolo emitido desde la fuente mediante la variable aleatoria $X : \scX\to\R$.
% Si noti che la scelta dei reali $a$ sui quali la variabile casuale $X$ mappa i simboli sorgente non è importante.
Fijado $D$ (base del código) indicamos con $\scD$ el conjunto $\{0,\dots,D-1\}$ de los símbolos de código con base $D$. Así, una función de codificación, o código, es una función del tipo $c : \scX\to\scD^+$.

Estamos preparados para definir formalmente el problema de la codificación fuente: dado un modelo de fuente $\langle\scX,p\rangle$ y una base $D > 1$, encontrar un código $c : \scX\to\scD^+$ tal que el valore medio
\begin{equation}
\label{eq:source-coding}
    \E\bigl[\ell_c\bigr] = \sum_{x\in\scX} \ell_c(x)\,p(x)
\end{equation}
de la longitud de la palabra de código sea mínimo.

Formulado en estos términos, el problema de la codificación fuente se presta a una solución banal e inútil. Es obvio que el código $c : \scX\to\scD^+$ tal que $c(x)=0$ para cada $x\in\scX$ minimiza $\E\bigl[\ell_c\bigr]$ para cada modelo de fuente. Por tanto, hace falta imponer limitaciones sobre la clase de códigos que queremos utilizar para resolver~(\ref{eq:source-coding}).

Una primera limitación es la siguiente. Un código $c : \scX\to\scD^+$ es \textbf{no singular} si a símbolos fuente distintos les corresponden palabras de código distintas. Formalmente, para cada $x,x'\in\scX$ tal que $x \neq x'$ se cumple $c(x) \neq c(x')$. Dicho de otra forma, la no singularidad del código es equivalente a la inyectividad de la función de codificación. Esta es claramente una propiedad necesaria para un código utilizable en la práctica.

Ahora introducimos un concepto natural: el de \textbf{extensión de un código}. La extensión se usa para definir de forma simple la palabra de código asociada a un mensaje de longitud dada, es decir, a una secuencia de símbolos fuente. Dado un código $c : \scX\to\scD^+$, su extensión es la función $C : \scX^+\to\scD^+$ definida come
$
    C(x_1,\dots,x_n) = c(x_1) \cdots c(x_n)
$, donde $c(x_1) \cdots c(x_n)$ indica la secuencia obtenida yuxtaponiendo las palabras de código $c(x_1),\dots,c(x_n)$.
%
\begin{example}
\label{ex:2}
La extensión $C$ del código definido en el Ejemplo~\ref{ex:1} es tal que
\[
    C(\heartsuit,\spadesuit,\clubsuit) = c(\heartsuit)c(\spadesuit)c(\clubsuit) = 01001~.
\]
\end{example}
%
La propiedad de no singularidad no es suficientemente fuerte para garantizar que esta sea heredada también por la extensión de un código. De hecho, la extensión en el Ejemplo~\ref{ex:2} es tal que
\[
    C(\diamondsuit) = C(\clubsuit,\heartsuit) = C(\heartsuit,\spadesuit) = 010~.
\]
Por tanto, mientras el código $c$ del Ejemplo~\ref{ex:1} es no singular su extensión $C$ no lo es.

Motivados por este ejemplo, introducimos la noción de código \textbf{unívocamente decodificable}, es decir, de código cuya extensión es no singular. Formalmente, $c$ es unívocamente decodificable si $C$ es una función inyectiva. En la práctica, esta propiedad permite decodificar los mensajes. De hecho, si $c$ es unívocamente decodificable entonces para cada $\by\in\scD^+$ encontraré como mucho un único mensaje $\bx\in\scX^+$ (la decodificación de $\by$) tal que $C(\bx) = \by$. La verficación para determinar si un código dado $c$ es unívocamente decodificable la realiza el algoritmo de Sardinas-Patterson en tiempo $\mathcal{O}(mL)$, donde $m$ es el número de las palabras de código y $L$ es la suma de sus longitudes.

\end{document}
