% Author: Nicolò Cesa-Bianchi (2016)
% This document is licensed under Creative Commons BY-NC-SA.
\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage[spanish]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}

\newtheorem{theorem}{Teorema}
\newtheorem{lemma}[theorem]{Lema}
\newtheorem{defi}[theorem]{Definición}
\newtheorem{cor}[theorem]{Corolario}
\newtheorem{example}[theorem]{Ejemplo}
\newtheorem{fact}[theorem]{Proposición}

\newcommand{\figscale}[2]{\includegraphics[scale=#2,clip=false]{#1}}

\newlength{\parunit}
\setlength{\parunit}{3mm}

\setlength{\parindent}{0pt}
\setlength{\parskip}{\parunit}
\setlength{\belowdisplayskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}

\newcommand{\ssep}{\\[1.5\parunit]}
\newcommand{\msep}{\\[2.0\parunit]}

\newcommand{\scO}{\mathcal{O}}
\newcommand{\bg}{ \boldsymbol{g} }
\newcommand{\bu}{ \boldsymbol{u} }
\newcommand{\bv}{ \boldsymbol{v} }
\newcommand{\bw}{ \boldsymbol{w} }
\newcommand{\bx}{ \boldsymbol{x} }
\newcommand{\by}{ \boldsymbol{y} }
\newcommand{\bX}{ \boldsymbol{X} }
\newcommand{\bY}{ \boldsymbol{Y} }
\newcommand{\bz}{ \boldsymbol{z} }
\newcommand{\be}{ \boldsymbol{e} }
\newcommand{\bone}{ \boldsymbol{1} }
\newcommand{\bzero}{ \boldsymbol{0} }
\newcommand{\bphi}{ \boldsymbol{\phi} }
\newcommand{\bxi}{ \boldsymbol{\xi} }
\newcommand{\bsigma}{ \boldsymbol{\sigma} }
\newcommand{\bbw}{\overline{\bw}}
\newcommand{\scD}{\mathcal{D}}
\newcommand{\scH}{\mathcal{H}}
\newcommand{\scF}{\mathcal{F}}
\newcommand{\scM}{\mathcal{M}}
\newcommand{\scP}{\mathcal{P}}
\newcommand{\scR}{\mathcal{R}}
\newcommand{\scX}{\mathcal{X}}
\newcommand{\scY}{\mathcal{Y}}
\newcommand{\yhat}{\widehat{y}}
\newcommand{\Yhat}{\widehat{Y}}
\newcommand{\hhat}{\widehat{h}}
\newcommand{\fhat}{\widehat{f}}
\newcommand{\defeq}{\stackrel{\rm def}{=}}
\newcommand{\ol}{\overline}
\newcommand{\loss}{\ell}
\newcommand{\er}{\mathrm{er}}
\newcommand{\sgn}{\mathrm{sgn}}
\renewcommand{\ss}{\subseteq}
\newcommand{\bool}{\{0,1\}}
\newcommand{\spin}{\{-1,+1\}}
\newcommand{\norm}[1]{ \left\|{#1}\right\| }
\newcommand{\theset}[2]{ \left\{ {#1} \,:\, {#2} \right\} }
\newcommand{\field}[1]{\mathbb{#1}}
\renewcommand{\Pr}{\field{P}}
\newcommand{\R}{\field{R}}
\newcommand{\N}{\field{N}}
\newcommand{\E}{\field{E}}
\newcommand{\Ind}[1]{\field{I}{\{{#1}\}}}
\newcommand{\ve}{\varepsilon}
\newcommand{\dt}{\displaystyle}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}


\begin{document}

\begin{center}
\fbox{
\begin{minipage}{0.9\textwidth}
\textbf{Teoría de la Información y de la Transmisión}
\begin{center}
\textbf{\Large Codificación de la fuente: definiciones}
\end{center}
Docente: \textit{Nicolò Cesa-Bianchi} \\ 
Traducción: \textit{Mario Román} \\
Licencia: Creative Commons BY-SA-NC \hfill versión \today 
\end{minipage}
}
\end{center}

\bigskip

Los mensajes a transmitir son generados desde una entidad abstracta llamada fuente. Sea $\scX$ el conjunto finito de símbolos que componen los mensajes generados por la fuente. Un mensaje $\bx = (x_1,\dots,x_n) \in \scX^n$ de longitud $n$ es una secuencia de $n$ símbolos fuente. Una función de codificación leva símbolos fuente a palabras de código. Una palabra de código es una secuencia de números del conjunto $\{0,\dots,D-1\}$ de los símbolos de código, donde $D > 1$ es la base del código. Por ejemplo, con $D=2$ obtenemos los códigos binarios. En este sentido, podemos representar una función de codificación por un código fuente con

\[
    c : \scX \to \{0,\dots,D-1\}^+
\]
donde $\{0,\dots,D-1\}^+$ representa el conjunto de las secuencias sobre $\{0,\dots,D-1\}$ de longitud mayor o igual a uno. Formalmente,
\[
    \{0,\dots,D-1\}^+ = \bigcup_{n=1}^{\infty} \{0,\dots,D-1\}^n~.
\]
Por ejemplo, $(2,1)$ y $(4,7,3)$ pertenecen ambas a $\{0,\dots,D-1\}^+$ con $D=8$.
%
\begin{example}
\label{ex:1}
Dado $\scX = \{\heartsuit,\diamondsuit,\clubsuit,\spadesuit\}$, un ejemplo de código binario $c : \scX\to\{0,1\}^+$ es el siguiente:
\[
    c(\heartsuit) = 0 \quad c(\diamondsuit) = 010 \quad c(\clubsuit) = 01 \quad c(\spadesuit) = 10~.
\]
\end{example}
%
Dado que el objetivo de un código fuente es maximizar la compresión, estamos interesados en medir la cantidad $\ell_c(x)$ definida como la longitud de la palabra de código $c(x)$ para el símbolo $x\in\scX$. En el ejemplo precedente, $\ell_c(\diamondsuit) = 3$.
%Definiamo anche la lunghezza totale della codifica di $\bx = (x_1,\dots,x_n) \in \scX^n$ come
%\[
%    L_c(\bx) = \sum_{i=1}^n \ell_c(x_i)~.
%\]
El objetivo de un código fuente es el de minimizar la longitud media de la palabra de código utilizada para codificar los símbolos fuente.

La intución en la base de la construcción de códigos fuente es la misma del alfabeto Morse: utilizar palabras de código cortas para los símbolos que son generados frecuentemente por la fuente. Para poder analizar de modo riguroso debemos crear un modelo formal de la fuente. La propuesta de Shannon es la de definir una distribución de probabilidad $p$ fijada sobre símbolos fuente y entonces asumir que $p(x)$ represente la probabilidad de que la fuente genere el símbolo $x\in\scX$. Un \textbf{modelo de fuente} está entonces definido por la pareja $\langle\scX,p\rangle$.

Nótese que, en realidad, la cantidad interesante aquí es la distribución de probabilidad sobre los mensajes $\bx$ más que sobre los símbolos $x$, dado que los mensajes son los objetos que queremos transmitir. Dada una distribución $p$ sobre símbolos $\scX$ definimos entonces una distribución $P_n$ sobre los mensajes $\scX^n$ de longitud $n$ como
\[
    P_n(x_1,\dots,x_n) = p(x_1) \times\cdots\times p(x_n)~.
\]
Esta definición de $P_n$ corresponde a asumir que la fuente genera un mensaje a través de extracciones indipendendientes de símbolos. En generale, sin embargo, esta asunción no es muy plausibile. Por ejemplo, pensemos en un mensaje de texto en castellano donde los símbolos fuente son las letras del alfabeto incluyendo espacios y punto. Claramente, hay fuertes dependencias entre una letra del mensaje y las letras que están alrededor y tales dependencias no son capturadas por la $P_n$ definida como antes. Por otro lado, el análisis matematico se ve muy facilitado por la asunción de independencia. En lo que sigue, asumiremos entonces la independencia de los símbolos fuente, teniendo sin embargo en mente que códigos fuente más realistas y sofisticados pueden obtenerse sin esta asunción.

De ahora en adelante identificamos un símbolo emitido desde la fuente mediante la variable aleatoria $X : \scX\to\R$.
% Si noti che la scelta dei reali $a$ sui quali la variabile casuale $X$ mappa i simboli sorgente non è importante.
Fijado $D$ (base del código) indicamos con $\scD$ el conjunto $\{0,\dots,D-1\}$ de los símbolos de código con base $D$. Así, una función de codificación, o código, es una función del tipo $c : \scX\to\scD^+$.

Estamos preparados para definir formalmente el problema de la codificación fuente: dado un modelo de fuente $\langle\scX,p\rangle$ y una base $D > 1$, encontrar un código $c : \scX\to\scD^+$ tal que el valore medio
\begin{equation}
\label{eq:source-coding}
    \E\bigl[\ell_c\bigr] = \sum_{x\in\scX} \ell_c(x)\,p(x)
\end{equation}
de la longitud de la palabra de código sea mínimo.

Formulado en estos términos, el problema de la codificación fuente se presta a una solución banal e inútil. Es obvio que el código $c : \scX\to\scD^+$ tal que $c(x)=0$ para cada $x\in\scX$ minimiza $\E\bigl[\ell_c\bigr]$ para cada modelo de fuente. Por tanto, hace falta imponer limitaciones sobre la clase de códigos que queremos utilizar para resolver~(\ref{eq:source-coding}).

Una prima limitazione è la seguente. Un codice $c : \scX\to\scD^+$ è \textbf{non singolare} se a simboli sorgente distinti corrispondono parole di codice distinte. Formalmente, per ogni $x,x'\in\scX$ tale che $x \neq x'$ vale $c(x) \neq c(x')$. In altre parole, la non singolarità del codice è equivalente all'iniettività della funzione di codifica. Questa è chiaramente una proprietà minimale per un codice utilizzabile in pratica.

Ora introduciamo un concetto naturale: quello di \textbf{estensione di un codice}. L'estensione serve a definire in modo semplice la parola di codice associata ad un messaggio di una data lunghezza, ovvero ad una sequenza di simboli sorgente. Dato un código $c : \scX\to\scD^+$, la sua estensione è la funzione $C : \scX^+\to\scD^+$ definita come
$
    C(x_1,\dots,x_n) = c(x_1) \cdots c(x_n)
$, dove $c(x_1) \cdots c(x_n)$ indica la sequenza ottenuta giustapponendo le parole di codice $c(x_1),\dots,c(x_n)$.
%
\begin{example}
\label{ex:2}
L'estensione $C$ del codice definito nell'Esempio~\ref{ex:1} è tale che
\[
    C(\heartsuit,\spadesuit,\clubsuit) = c(\heartsuit)c(\spadesuit)c(\clubsuit) = 01001~.
\]
\end{example}
%
La proprietà di non singolarità non è abbastanza forte per garantire che essa venga ereditata anche dall'estensione di un codice. Infatti, l'estensione nell'Esempio~\ref{ex:2} è tale che
\[
    C(\diamondsuit) = C(\clubsuit,\heartsuit) = C(\heartsuit,\spadesuit) = 010~.
\]
Quindi mentre il codice $c$ dell'Esempio~\ref{ex:1} è non singolare la sua estensione $C$ non lo è.

Motivati da questo esempio, introduciamo la nozione di codice \textbf{univocamente decodificabile}, ovvero di codice la cui estensione è non singolare. Formalmente, $c$ è univocamente decodificabile se $C$ è una funzione iniettiva. In pratica questa proprietà permette di decodificare i messaggi. Infatti, se $c$ è univocamente decodificabile allora per ogni $\by\in\scD^+$ trovo al più un unico messaggio $\bx\in\scX^+$ (la decodifica di $\by$) tale che $C(\bx) = \by$. La verifica per determinare se un dato codice $c$ sia univocamente decodificabile è realizzata dall'algoritmo di Sardinas-Patterson in tempo $\mathcal{O}(mL)$, dove $m$ è il numero delle parole di codice e $L$ è la somma delle loro lunghezze.

\end{document}
