\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage[spanish]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{graphicx}

\newtheorem{theorem}{Teorema}
\newtheorem{lemma}[theorem]{Lema}
\newtheorem{defi}[theorem]{Definición}
\newtheorem{cor}[theorem]{Corolario}
\newtheorem{example}[theorem]{Ejemplo}
\newtheorem{fact}[theorem]{Proposición}

\newenvironment{proof}{{\textsc{Demostración.}\ }}{\hfill$\Box$\\[2mm]}

\newcommand{\figscale}[2]{\includegraphics[scale=#2,clip=false]{#1}}

\newlength{\parunit}
\setlength{\parunit}{3mm}

\setlength{\parindent}{0pt}
\setlength{\parskip}{\parunit}
\setlength{\belowdisplayskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}

\newcommand{\ssep}{\\[1.5\parunit]}
\newcommand{\msep}{\\[2.0\parunit]}

\newcommand{\scO}{\mathcal{O}}
\newcommand{\bg}{ \boldsymbol{g} }
\newcommand{\bu}{ \boldsymbol{u} }
\newcommand{\bv}{ \boldsymbol{v} }
\newcommand{\bw}{ \boldsymbol{w} }
\newcommand{\bx}{ \boldsymbol{x} }
\newcommand{\by}{ \boldsymbol{y} }
\newcommand{\bX}{ \boldsymbol{X} }
\newcommand{\bY}{ \boldsymbol{Y} }
\newcommand{\bz}{ \boldsymbol{z} }
\newcommand{\be}{ \boldsymbol{e} }
\newcommand{\bone}{ \boldsymbol{1} }
\newcommand{\bzero}{ \boldsymbol{0} }
\newcommand{\bphi}{ \boldsymbol{\phi} }
\newcommand{\bxi}{ \boldsymbol{\xi} }
\newcommand{\bsigma}{ \boldsymbol{\sigma} }
\newcommand{\bbw}{\overline{\bw}}
\newcommand{\scD}{\mathcal{D}}
\newcommand{\scH}{\mathcal{H}}
\newcommand{\scF}{\mathcal{F}}
\newcommand{\scM}{\mathcal{M}}
\newcommand{\scP}{\mathcal{P}}
\newcommand{\scR}{\mathcal{R}}
\newcommand{\scX}{\mathcal{X}}
\newcommand{\scY}{\mathcal{Y}}
\newcommand{\yhat}{\widehat{y}}
\newcommand{\Yhat}{\widehat{Y}}
\newcommand{\hhat}{\widehat{h}}
\newcommand{\fhat}{\widehat{f}}
\newcommand{\lmax}{\ell_{\textrm{max}}}
\newcommand{\defeq}{\stackrel{\rm def}{=}}
\newcommand{\ol}{\overline}
\newcommand{\loss}{\ell}
\newcommand{\er}{\mathrm{er}}
\newcommand{\sgn}{\mathrm{sgn}}
\renewcommand{\ss}{\subseteq}
\newcommand{\bool}{\{0,1\}}
\newcommand{\spin}{\{-1,+1\}}
\newcommand{\norm}[1]{ \left\|{#1}\right\| }
\newcommand{\theset}[2]{ \left\{ {#1} \,:\, {#2} \right\} }
\newcommand{\field}[1]{\mathbb{#1}}
\renewcommand{\Pr}{\field{P}}
\newcommand{\R}{\field{R}}
\newcommand{\E}{\field{E}}
\newcommand{\Ind}[1]{\field{I}{\{{#1}\}}}
\newcommand{\ve}{\varepsilon}
\newcommand{\dt}{\displaystyle}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}


\begin{document}

\begin{center}
\fbox{
\begin{minipage}{0.9\textwidth}
\textbf{Teoría de la Información y de la Transmisión}
\begin{center}
\textbf{\Large Códigos instantáneos y la desigualdad de Kraft}
\end{center}
Docente: \textit{Nicolò Cesa-Bianchi} \\ 
Traducción: \textit{Mario Román} \\
Licencia: Creative Commons BY-SA-NC \hfill versión \today 
\end{minipage}
}
\end{center}

\bigskip

Retomemos nuestro objetivo de esta primera fase: la búsqueda de un código fuente óptimo. Es decir, dado un modelo de fuente $\langle\scX,p\rangle$ queremos encontrar el código $c : \scX\to\scD^+$ tal que el valor esperado
\begin{equation}
\label{eq:obj}
    \E\bigl[\ell_c\bigr] = \sum_{x\in\scX} \ell_c(x)\,p(x)
\end{equation}
de la longitud de palabra de código sea mínimo.

como ya hemos observado antes, para que un código $c$ cualquiera sea utilizable en la práctica, debe ser posible el proceso de decodificación de mensajes. Es decir, dada una palabra de código $\by\in\scD^+$ debe ser posible remontarse al mensaje $\bx\in\scX^+$ tal que $C(\bx)=\by$. Para evitar tener un código inutilizable como solución óptima, estamos por tanto restringidos a los códigos unívocamente decodificables. Es decir, los códigos cuya extensión no es singular. Por desgracia, también los códigos unívocamente decodificables pueden presentar problemas desde el punto de vista práctico, como se ve en el ejemplo siguiente.
%
\begin{example}
\label{ex:ud}
Dado $\scX = \{\heartsuit,\diamondsuit,\clubsuit,\spadesuit\}$, se considera el siguiente código binario unívocamente decodificable
\[
    c(\heartsuit) = 10 \quad c(\diamondsuit) = 00 \quad c(\clubsuit) = 11 \quad c(\spadesuit) = 110~.
\]
Supongamos ahora que hubiéramos utilizado la extensión $C$ para codificar un mensaje $\bx\in\scX^+$ y hubiéramos obtenido la palabra de código $110\dots 0$. Durante el proceso de decodificación, para entender si el primer símbolo del mensaje fuente es $\clubsuit$ en vez de $\spadesuit$ deberíamos verificar si el número de ceros que siguen a $11$ es par o impar. De hecho, si el número de ceros es par, entonces el mensaje debe ser de la forma $\clubsuit\diamondsuit\dots\diamondsuit$. En otro caso, si el número de ceros es impar, entonces el mensaje debe ser de la forma $\spadesuit\diamondsuit\dots\diamondsuit$.
\end{example}
%
El ejemplo precedente muestra que un código unívocamente decodificable puede ser tal que, para comenzar a decodificar el primer símbolo de un mensaje, debamos esperar a leer el último símbolo de su codificación. Examinando el código, nos damos cuenta de que el problema está en el hecho de que la palabra de código para $\clubsuit$ es un prefijo de la palabra de código para $\spadesuit$. Si ninguna palabra de código fuese prefijo de otra, entonces podríamos decodificar los símbolos fuente conforme recibiésemos los símbolos de código.

Nótese que podemos remediar este problema reservando un símbolo de código para separar las codificaciones de los símbolos fuente en una palabra de código. Por ejemplo, el código binario del Ejemplo~\ref{ex:ud} se convertiría en el siguiente código ternario
\[
    c(\heartsuit) = 102 \quad c(\diamondsuit) = 002 \quad c(\clubsuit) = 112 \quad c(\spadesuit) = 1102~.
\]
Sin embargo, es evidente que esta solución compromete la compacidad del código.

Un código $c : \scX \to \scD^+$ se llama \textbf{instantáneo} si ninguna palabra de código es prefijo de ninguna otra. Por ejemplo, el código binario
\[
    c(\heartsuit) = 0 \quad c(\diamondsuit) = 10 \quad c(\clubsuit) = 110 \quad c(\spadesuit) = 111~.
\]
es instantáneo.

Claramente, un código instantáneo es no singular. Mostramos ahora que los códigos instantáneos son un subconjunto de aquellos unívocamente decodificables.
%
\begin{fact}
si $c$ es instantáneo entonces es también unívocamente decodificable.
\end{fact}
%
\begin{proof}
Sea $c : \scX\to\scD^+$ un código cualquiera y sea $C$ su extensión. Sin pérdida de generalidad, podemos asumir que $c$ sea no singular. De hecho, si $c$ fuese singular, entonces no sería instantáneo. Demostramos que si $c$ no es unívocamente decodificable, entonces $c$ no puede ser instantáneo. Si $c$ no es unívocamente decodificable existen dos mensajes $\bx,\bx'\in\scX^+$ distintos tales que $C(\bx) = C(\bx')$. Solo hay dos formas en las que $\bx$ y $\bx'$ pueden ser distintos: (1) un mensaje es prefijo del otro; (2) hay al menos una posición en la que los dos mensajes difieren. Para analizar el primer caso, asumimos por ejemplo que $\bx'$ es un prefijo de $\bx$. Pero entonces, dado que $C(\bx) = C(\bx')$, los restantes símbolos de $\bx$ deberían ser mapados desde $c$ en la palabra de código vacía, lo que no es posible por nuestra definición de código.\footnote{Incluso si entendiésemos la definición de código admitiendo códigos que mapan símbolos fuente en la palabra vacía, tales códigos no serían instantáneos, en cuanto la palabra vacía sería prefijo de cualquier otra palabra de código.} Queda entonces analizar el segundo caso: hay una posición $i$ en la cual $\bx$ y $\bx'$ difieren por primera vez, es decir $x_i \neq x'_i$ y $x_j = x'_j$ para cada $j = 1,\dots,i-1$. Entonces tenemos $c(x_j)=c(x'_j)$ para $j=1,\dots,i-1$ y $c(x_i) \neq c(x'_i)$ dado que $c$ es no singular. Pero entonces la única forma de que $C(\bx) = C(\bx')$ es que $c(x_i)$ sea prefijo de $c(x'_i)$ o viceversa, lo que contradice la hipótesis de que $c$ es instantáneo.
\end{proof}
%
Hemos establecido así una jerarquía entre las funciones de la forma $c:\scX\to\scD^+$. Es decir,
\[
    \text{códigos instantáneos} \;\subset\; \text{códigos univ.\ decodificables} \;\subset\; \text{códigos no singulares}
\]
donde las inclusiones son estrictas. De hecho, los ejemplos precedentes han mostrado que existen códigos no singulares que no son unívocamente decodificables y códigos unívocamente decodificables que no son instantáneos.

Los códigos instantáneos satisfacen una importante propiedad estructural que los mantiene reconocibles también sólo en base a las longitudes de las palabras de código.
%
\begin{lemma}[Desigualdad de Kraft]
Dados $\scX = \{x_1,\dots,x_m\}$, $D > 1$ y $m$ enteros $\ell_1,\dots,\ell_m > 0$, existe un código instantáneo $c : \scX \to \scD^+$ tal que $\ell_c(x_i) = \ell_i$ para $i=1,\dots,m$ si y sólo si
\[
    \sum_{i=1}^m D^{-\ell_i} \le 1~.
\]
\end{lemma}
%
\begin{proof}
Comenzamos a demostrar que dado $c$ instantáneo, las longitudes de sus palabras de código obedecen la desigualdad de Kraft. Ses $\lmax$ la longitud máxima de las palabras de $c$,
\[
    \lmax = \max_{i=1,\dots,m} \ell_c(x_i)~.
\]
Considérese el árbol $D$-ario completo de profundidad $\lmax$. Podemos posicionar cada palabra de código de $c$ sobre un nodo del árbol siguiendo desde la raíz el camino correspondiente a los símbolos de la palabra. Dado que el código es instantáneo, ninguna palabra pertenecerá al subárbol teniendo como raíz otra palabra. Podemos entonces particionar las hojas del árbol en subconjuntos disjuntos $A_1,\dots,A_m$, donde $A_i$ es el subconjunto de hojas asociado a la palabra $c(x_i)$ ---véase la Figura~\ref{fig:kraft}.
%
\begin{figure}[h]
\begin{center}
\figscale{Images/kraft}{0.4}
\end{center}
\caption{
\label{fig:kraft}
Partición de las hojas inducida desde el código instantáneo indicado por los nodos coloreados (de Wikipedia). La demostración de la desigualdad de Kraft puede utilizar un árbol binario completo cualquiera de profundidad mayor o igual a $\lmax$. Podemos entonces sustituir el árbol de la figura por un árbol binario completo de profundidad $\lmax = 3$.
}
\end{figure}

Ahora, el número de hojas en el subárbol de una palabra de altura $\ell_i$ es obviamente $D^{\lmax-\ell_i}$. Por otro lado, el número total de hojas en el árbol es $D^{\lmax}$. Por tanto,
\[
    \sum_{i=1}^m D^{\lmax-\ell_i} = \sum_{i=1}^m |A_i| \le D^{\lmax}~.
\]
Dividiendo por $D^{\lmax}$ el miembro izquierdo y el de la derecha de la fórmula obtenemos la desigualdad de Kraft.

Para demostrar la otra implicación asumimos que $\ell_1,\dots,\ell_m > 0$ satisfacen la desigualdad de Kraft, y sea $\lmax = \max\{\ell_1,\dots,\ell_m\}$. Entonces podemos construir un código instantáneo $c : \{x_1,\dots,x_m\} \to \scD^+$ con longitudes dadas, es decir, $\ell_c(x_i) = \ell_i$ para $i=1,\dots,m$. A este propósito, considérese el árbol $D$-ario ordenado y completo de profundidad $\lmax$. Al símbolo $x_1$ le asociamos la palabra de código $c(x_1)$ correspondiente al nodo del árbol de altura $\ell_1$ primero en orden lexicográfico (es decir, más a la izquierda). A cada símbolo sucesivo $x_i$, asociamos la palabra de código $c(x_i)$ correspondiente al primer nodo (siempre en orden lexicográfico) de altura $\ell_i$ que ni pertenezca ni incluya subárboles radicados sobre palabras elegidas previamente ---véase nuevamente la Figura~\ref{fig:kraft}. Nótese que el código así construido es instantáneo, dado que ninguna palabra aparecerá en el subárbol radicado sobre otra palabra. Dado que las longitudes satisfacen la desigualdad de Kraft, el número total de hojas necesarias para crear el código es
\[
    \sum_{i=1}^m D^{\lmax-\ell_i} \le D^{\lmax}
\]
es decir, no mayor de las hojas disponibles en el árbol.
\end{proof}
Veamos ahora cómo construir un buen código instantáneo, es decir, un código que tienda a minimizar~(\ref{eq:obj}). Fijado $D > 1$ y un modelo de fuente $\langle X,p \rangle$, la desigualdad de Kraft nos dice que podemos limitarnos a buscar números positivos $\ell_1,\dots,\ell_m$ que la satisfagan. De hecho, una vez encontrados, podemos construir automáticamente un código instantáneo con esas longitudes. Por tanto, debemos resolver el problema
\[
    \left\{ \begin{array}{l}
       {\dt \min_{\ell_1,\dots,\ell_m} \sum_{i=1}^m \ell_i\,p_i }
    \\[2mm]
        \text{tal que} {\dt \quad \sum_{i=1}^m D^{-\ell_i} \le 1 }
    \end{array} \right.
\]
donde $p_i = p(x_i)$ para $i=1,\dots,m$.
 
Podemos ahora observar que, dado que $p_1+\cdots+p_m = 1$ (es una distribución de probabilidad), podemos poner $D^{-\ell_i} \le p_i$ de forma que
\[
    \sum_{i=1}^m D^{-\ell_i} \le \sum_{i=1}^m p_i = 1
\]
satisfaciendo así la desigualdad de Kraft. Resolviendo para $\ell_i$ obtenemos la condición
\[
    \ell_i \ge \log_D\frac{1}{p_i}~.
\]
Por tanto, es suficiente tener $\ell_i = \bigl\lceil \log_D\tfrac{1}{p_i} \bigr\rceil$ para tener enteros que satisfagan la desigualdad de Kraft. El código instantáneo resultante se conoce como \textbf{código de Shannon}.

Es interesante estudiar el caso particular en el que los $p_i$ sean inversos de potencias de $D$. Por Ejemplo, cuando $p_1 = \tfrac{1}{2}$, $p_2 = \tfrac{1}{4}$, $p_3 = \tfrac{1}{8}$, $p_4 = \tfrac{1}{8}$ para $D=2$ e $m=4$. En estos casos, la longitud del código de Shannon puede ser calculada exactamente como
\[
    \sum_{i=1}^m \ell_i\,p_i = \sum_{i=1}^m p_i\log_D\frac{1}{p_i}~.
\]
Esta cantidad, que es una propiedad sólo de la distribución $p_1,\dots,p_m$, se conoce como \textbf{entropía}. Las relaciones entre entropía y codificación óptima vendrán profundizadas en las siguientes lecciones.

\end{document}
